\documentclass{article}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{listings}

% TODO: evaluate entropy for the generated text!!!
% TODO: alpha difference when it is below 0?
% TODO: the lesser the K, the more time the MostCommonPrediction manager takes
% TODO: final conclusion: we didn't take into account *when* we should start copying, since we copy as soon we can
% TODO: due to time constraints, the thresholds were not evaluated in conjunction

\begin{document}

\title{
    Assignment 1 - Copy models for data compression \\
    \large{Algorithmic Information Theory (2022/23) \\
    Universidade de Aveiro}
}

\author{
    Martinho Tavares, 98262, martinho.tavares@ua.pt \and
    Nuno Cunha, 98124, nunocunha@ua.pt \and
    Pedro Lima, 97860, p.lima@ua.pt
}

\date{\today}
\maketitle

\section{Introduction}

One of the goals of this assignment is to implement a copy model for data compression, which is obtained through the exploration of self similarities.
A copy model is only one of the ways to address the problem, and its idea is that certain data sources can be viewed as a sequence of repetitive patterns,
where parts of the data have been replicated in the past, although some modifications are possible. 
The model consists in predicting the next symbol based on previous ones observed in the past, by keeping a pointer referring to the symbol being copied, as well as other information.

The second goal of the assignment is to implement a text generator based on the copy model, which is a way to generate new text based on a given one.
The text generator receives a text as input to train the model, and then follows a similar approach to the one used in the copy model,
but instead of predicting the next symbol, it uses the probability distribution to generate a random symbol based on these probabilities.

In this report we will first present how the work was organized, in the \ref{sec:work-organization} section.
Then, we will present the \ref{sec:copy-model}, how we implemented it and the results obtained, which we will compare by calculating the entropy of different text examples,
like \verb|chry.txt| given by the teachers, with different parameters we defined for the model.
The next section is dedicated to the \ref{sec:text-generator}, where we will present the implementation and the results obtained for the text generator,
using different texts as input for training of the generator, as well as diferrent starting points for the generation of text.
Finally, we will conclude the report in the \ref{sec:conclusion} section.

\section{Work organization}
\label{sec:work-organization}

To organize the work we decide to use a object oriented approach, this way it was easier to implement different parameters to test the copy model and the text generator.

First for the copy model we created a file named \verb|main.cpp|, where we instantiate the object \verb|CopyModel|, which is the class that implements the copy model.
The \verb|CopyModel| class has a constructor that receives the parameters that we want to test, and with these parameters we can instantiate different objects of certain classes, and test them with different files.
The more important classes are the abstract classes: \verb|CopyPointerManager|, \verb|CopyPointerThreshold| and \verb|BaseDistribution|. By using these base classes we could implement different strategies for the copy model, by creating child classes of these base classes, and then instantiate the \verb|CopyModel| object with the child classes as parameters.
This proved to be a very useful approach, since we could test different strategies for the copy model, and compare the results obtained to see which strategy, or combination of strategy/parameters, was the best.

For the text generator all the code is in the \verb|cpm_gen.cpp| file, due to its more simple structure. The generator also implements some parameters and uses a similar strategy of reading patterns as the copy model.

\section{Copy model}
\label{sec:copy-model}

The copy model behaves in a straightforward way.
At each symbol in the file, the program verifies if the pattern of $k$ symbols with the current symbol being the last has already occurred in the past.
That is, at position $x_n$, the pattern within positions $x_{n-k+1}$ and $x_n$ is evaluated to see if it appeared in the past.

To do that, each of these $k$-sized patterns at every position is saved in a hash table built incrementally while the file is being processed.
The keys are the $k$mers/patterns, and the values are arrays of positions in the file where those $k$mers occurred in the past, which we call pointers.
When we need to choose a pattern in the past, we look through the array of pointers for that pattern for a specific occurrence of that pattern.

Whenever the pattern is still not present in the hash table, we try to randomly guess which symbol should occur next since we can't perform a copy from the past.
In these cases, the reported probabilities follow a fixed default distribution that is set beforehand.

When predictions can be made, a strategy is followed to choose the appropriate pointer for the current pattern.
After a pointer is chosen, it is fixed and predictions are made sequentially from that point on.
That is, if at position $x_n$ we have the copy pointer $x_m$, then we will predict the character at $x_{n+1}$ to be equal to the character at $x_{m+1}$, the character at $x_{n+2}$ to be equal to the character at $x_{m+2}$, and so on.
The predictions aren't completely certain; we assume a probability of making a correct prediction, which varies depending on how well the prediction going.

This process is repeated until a threshold is met, which is based on the probability of making a correct prediction at the current position.
When the threshold is met, we stop predicting, and keep evaluating the file's content as normal.
If the current $k$mer has already occurred in the past, then we attempt a new chain of predictions.
Otherwise, we perform guesses.

We need to be careful regarding the beginning of the file.
For the first $k-1$ symbols, it is not possible to generate a $k$mer, but we should still be able to perform predictions starting on these $k-1$ symbols.
We followed a simple strategy, which involves extending the file content backwards, generating a past of size $k$.
The generated past is simple, it is composed of $k$ repetitions of a single symbol of the alphabet.
This symbol is the most frequent of the entire alphabet.

In order to evaluate whether the copy model can provide acceptable results, we can use a baseline below which we expect the model to report the file's entropy.
We decided to use, as a baseline, the entropy considering each symbol's relative frequency in the entire file in bits, which is given by:

$$
H(X) = - \sum_{x \in X}{p(x) \log_{2}{p(x)}}
$$

With this value in mind, we evaluated the model as a whole with different values for its parameters, on different files.
The files chosen for testing are present in the repository\footnote{\url{https://github.com/NMPC27/TAI-G7-Lab1}}, and they have the following baselines:
\begin{itemize}
    \item \verb|chry.txt|: 1.9652 bits
    \item \verb|othello.txt|: 4.44629 bits
\end{itemize}

Throughout this section, the different program parameters are detailed, and their effect on the model's performance, both in terms of execution time and quality of the results, is studied.
To analyze the effect of a parameter $l$, the copy model is run with different values for $l$ while keeping all other parameters with the default values.

For the results regarding the information content of the symbols throughout the \verb|chry.txt| and \verb|othello.txt| files, the data was transformed with a low-pass filter using a moving average with window size of 10000 and 100 samples respectively, since the data exhibited a very large frequency.
Additionally, the file \verb|othello.txt| was altered to not include newlines or commas, which were respectively replaced by spaces and semicolons.
These were complicating posterior data analysis due to how the model's results are saved to disk, which use the verbose machine mode described in section \ref{sec:copy-model-verbose}.

\subsection{Pattern size}

When choosing a pointer in the past from which to start copying, we need to look for an occurrence of the same $k$-sized pattern as the one we are currently on.
Thus, $k$ is one of the parameters that affects program performance, where $k$ is a positive integer.

On one hand, a lower value of $k$ means there will be more occurrences of each pattern, since there will be lesser possible $k$-sized patterns, which will result in more oportunities to copy from the past.
On the other, a larger $k$ will cause less occurrences for each pattern, which results in more scarce predictions, and thus more overall guesses.

For a given $k$, the number of possible patterns $M$ is equal to the number of permutations of the symbols of alphabet $A$: $M = \vert A \vert ^ k$.
The results on changing this value will be studied taking this exponential growth into account.

\subsubsection{Results}

\dots

\subsection{Smoothing parameter alpha}

At each coding step, the probability of correctly predicting the current character is given by the formula:

$$
P(\textnormal{hit}) = \frac{N_h + \alpha}{N_h + N_f + 2\alpha}
$$

where $N_h$ and $N_f$ are respectively the number of hits (correct predictions) and misses (wrong predictions), and $\alpha$ is the smoothing parameter.
The smoothing parameter is used to avoid the model having a probability of 0 when there are no hits or misses, which will happen when no prediction has been made yet, and also avoid assuming with complete certainty (probability of 1) that the prediction will be correct when no fails were made yet, which will result in infinite information in case the prediction fails.

If $\alpha$ is lower, we expect the reported probabilites to be more ``certain'' with a greater possibility of reaching high values.
If $\alpha$ is greater, the smoothing effect should be more pronounced, producing probability values that hardly increase, which should result in relatively less information when predictions fail but also lesser returns on the correct predictions.

This trade-off between $\alpha$ and the reported information, especially regarding the information spikes that may occur, will be studied in the results.

\subsubsection{Results}

Runs were performed with $\alpha \in {0.25, 0.5, 1.0, 2.0, 4.0, 8.0}$, where the next value is double the previous value.
The results on the information of the current symbol in the message and the reported file entropy at each step in the coding process are
present in figure \ref{fig:results-alpha}, for file \verb|chry.txt|.
For file \verb|othello.txt|, the results are in figure \ref{fig:results-alpha-other}.

\begin{figure}
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_0.25_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 0.25$}
        \label{fig:results-alpha-0.25}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_0.5_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 0.5$}
        \label{fig:results-alpha-0.5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_1.0_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 1.0$}
        \label{fig:results-alpha-1.0}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_2.0_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 2.0$}
        \label{fig:results-alpha-2.0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_4.0_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 4.0$}
        \label{fig:results-alpha-4.0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_8.0_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 8.0$}
        \label{fig:results-alpha-8.0}
    \end{subfigure}
    \caption{Information of the current symbol and entropy as the file is processed, varying the $\alpha$ parameter on ``chry.txt''.}
    \label{fig:results-alpha}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/othello_12_0.25_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 0.25$}
        \label{fig:results-alpha-other-0.25}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/othello_12_0.5_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 0.5$}
        \label{fig:results-alpha-other-0.5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/othello_12_1.0_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 1.0$}
        \label{fig:results-alpha-other-1.0}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/othello_12_2.0_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 2.0$}
        \label{fig:results-alpha-other-2.0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/othello_12_4.0_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 4.0$}
        \label{fig:results-alpha-other-4.0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/othello_12_8.0_f_m_[f:6,].png}
        \end{center}
        \caption{$\alpha = 8.0$}
        \label{fig:results-alpha-other-8.0}
    \end{subfigure}
    \caption{Information of the current symbol and entropy as the file is processed, varying the $\alpha$ parameter on ``othello.txt''.}
    \label{fig:results-alpha-other}
\end{figure}

From the graphs, we can notice that for $\alpha < 1.0$ the behavior of the model is drastically different, in subfigures \ref{fig:results-alpha-0.25}, \ref{fig:results-alpha-0.5}, \ref{fig:results-alpha-0.25-other} and \ref{fig:results-alpha-0.5-other}.
For \verb|chry.txt|, when $\alpha = 0.5$ the reported entropy in the middle of the file is considerably greater than when $\alpha = 0.25$, although the final reported entropy is the same.
Additionally, the reported information at the last steps is extremely low when compared with the symbol information in previous steps, which means a very good prediction was being performed.
However, we couldn't draw an explanation for why this is the case, since in the implementation the \verb|double| storage is used for $\alpha$ in the whole program and the prediction probability formula doesn't hint at a major difference in reported values when $\alpha < 1.0$.

When $\alpha >= 1.0$ in \verb|chry.txt|, the difference between runs can be seen on the reported information, especially in the middle of the processing, where the reported symbol information is lower when predicting if $\alpha$ is closer to 1.
This makes sense, since the prediction probability is less smoothed and thus is allowed to reach higher values, which in turn results in reporting less information when the predictions are correct.
However, it is also expected that as $\alpha$ increases, less information is reported when the prediction fails, but this is less clearly seen in the plots.

Another observation is that, as $\alpha$ increases, the entropy within steps $1.5\times10^7$ and $2.0\times10^7$.

\subsection{Base probability distribution}

For each symbol in the file, a probability distribution has to be generated according to the prediction probability.
In case a given prediction fails, we need to distribute the complement of the probability of a correct prediction $1 - P(\textnormal{hit})$ among the other symbols of the alphabet, except the one that we predicted would occur (which has probability $P(\textnormal{hit})$).

The choice of distribution also influences the guesses that are made.
When a prediction can't be made, the model assumes a distribution for the entirety of the alphabet, which is the same probability distribution as the use used when distributing the complement of the prediction probability.

We developed two simple approaches to distributing the probability: a uniform distribution and a frequency distribution.

The uniform distribution assigns to each symbol a probability equal to:

$$
\frac{1 - P(\textnormal{hit})}{\vert A \vert - 1}
$$

On the other hand, the frequency distribution assigns to each symbol $a \in A$ a probability equal to:

$$
(1 - P(\textnormal{hit})) \times p(a)
$$

where $p(a)$ is the probability of $a$ in the whole file.

Both distributions are compared especially in their effect on the overall reported information, when $k$ is large and thus more guesses are made, as well as when $\alpha$ is large and the prediction fails should report.
We assume that, since the frequency distribution models the overall distribution of the symbols, it should provide better results.
We should note, however, that the file's content may have locality, i.e. select portions in the file where the symbol distribution is skewed.
Therefore, we don't disregard the possibility that the assumption of the distribution being maintained throughout the file can hurt the model performance as well.

\subsubsection{Results}

Figures \ref{fig:results-distribution} and \ref{fig:results-distribution-other} show the symbol information and entropy throughout the processing of both test files, when using either the uniform or frequency distribution.

\begin{figure}
    \begin{subfigure}[b]{0.45\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_1.0_u_m_[f:6,].png}
        \end{center}
        \caption{Uniform distribution.}
        \label{fig:results-distribution-uniform}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_1.0_f_m_[f:6,].png}
        \end{center}
        \caption{Frequency distribution.}
        \label{fig:results-distribution-frequency}
    \end{subfigure}
    \caption{Information of the current symbol and entropy as the file is processed, varying the base distribution on ``chry.txt''.}
    \label{fig:results-distribution}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{0.45\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/othello_12_1.0_u_m_[f:6,].png}
        \end{center}
        \caption{Uniform distribution.}
        \label{fig:results-distribution-other-uniform}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/othello_12_1.0_f_m_[f:6,].png}
        \end{center}
        \caption{Frequency distribution.}
        \label{fig:results-distribution-other-frequency}
    \end{subfigure}
    \caption{Information of the current symbol and entropy as the file is processed, varying the base distribution on ``othello.txt''.}
    \label{fig:results-distribution-other}
\end{figure}

For \verb|chry.txt|, the effect of the distributions is minimal, but noticeable.
More specifically, the frequency distribution exhibited a lower and more stable entropy value throughout the file.
In terms of symbol information, the runs with the uniform distribution presented very similar values of information when predictions fail (data points above 2 bits), while for those with the frequency distribution the information values on failed predictions are more irregular throughout the file.
This is due to the fact that the frequency distribution assigns for each symbol of the alphabet a different probability by default, relative to the symbol's frequency in the whole file, and so different values are provided throughout the file which model the symbol's appearance in the file's contents.

For \verb|othello.txt|, however, the frequency distribution provided much better entropy.
The effect of this distribution is more pronounced in \verb|othello.txt| since the size of the alphabet is larger and the probability of occurrence between symbols is greatly skewed, as in the english alphabet there are characters that appear much more often than others.
The uniform distribution didn't take this into account, and thus reported larger values for the entropy.
One unexplained detail, however, is how for the run using the frequency distribution the reported entropy in the first few steps is abnormally high.

As such, the previously mentioned possibility that the frequency distribution may also hurt model performance was not found in these results.

\subsection{Copy pointer repositioning strategy}

Different strategies can be followed when choosing a copy pointer from which to start copying.
We developed 3 approaches, 2 of which being fairly simplistic:
\begin{itemize}
    \item \textbf{Oldest pointer first}: the chosen copy pointer is the oldest possible. If the current copy pointer is repositioned because the threshold was met, then that pointer will be completely discarded and the next oldest pointer is used.
    This is the most simple approach, which assumes similar patterns are very far in the file;
    \item \textbf{Newest pointer first}: the chosen copy pointer is always the newest, which assumes that similar patterns are very close in the file;
    \item \textbf{Most common prediction}: when a pointer has to be chosen, the predictions of all registered pointers are evaluated, and the pointer (or one of them) which presented longest and most common prediction is chosen.
    A more detailed description of this strategy is presented below.
    \item \textbf{Most common prediction bounded}: the approach is exactly the same as the \textbf{most common prediction}, with the only difference being that the registered pointers for each pattern are stored in a circular array with limited size.
\end{itemize}

The \textbf{most common prediction} strategy chooses the copy pointer in successive iterations.
Initially, the next immediate character is evaluated for every registered pointer.
The pointers are grouped according to that character that they predict, and the largest group is chosen, i.e. the pointers that reported the most common prediction are chosen.
Afterwards, the second next character is evaluated for every pointer in that group, and then the pointers are grouped in the same manner again, with the next group being the largest at this stage.
This process is repeated until all pointers at the current group report different predictions, at which point the newest pointer is chosen.

In order to obtain the most common prediction at each iteration, the Boyer-Moore majority vote algorithm was used \cite{majority}.
This is a very space-efficient algorithm (complexity $O(1)$) that guarantees the majority value will be returned.
However, if a majority value doesn't actually exist then the algorithm may erroneously report a value as the majority, if we only do one pass.
Since in this case we don't have a specific disambiguation strategy when the pointers don't agree, the choice is arbitrary and thus this is not a problem.

The bounded version of \textbf{most common prediction} uses a circular array for the set of registered pointers.
The idea is for the array of registered pointers to contain only the $N$ most recent pointers, with $N$ being the array size.
This can not only take into account the locality that the file's content may have, i.e. the repeated patterns being close to each other, but it also bounds the array size to a fixed value, which helps in performance since the reposition algorithm previously described has linear time complexity $O(N)$, when the other strategies have only $O(1)$ complexity.

\subsubsection{Results}

Figures \ref{fig:results-reposition} and \ref{fig:results-reposition-other} show the results for the symbol information and entropy throughout both files.

\begin{figure}
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_1.0_f_n_[f:6,].png}
        \end{center}
        \caption{Oldest pointer first.}
        \label{fig:results-reposition-n}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_1.0_f_o_[f:6,].png}
        \end{center}
        \caption{Newest pointer first.}
        \label{fig:results-reposition-o}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_1.0_f_m_[f:6,].png}
        \end{center}
        \caption{Most common prediction.}
        \label{fig:results-reposition-m}
    \end{subfigure}
    \caption{Information of the current symbol and entropy as the file is processed, varying the pointer reposition strategy.}
    \label{fig:results-reposition}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_1.0_f_n_[f:6,].png}
        \end{center}
        \caption{Oldest pointer first.}
        \label{fig:results-reposition-other-n}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_1.0_f_o_[f:6,].png}
        \end{center}
        \caption{Newest pointer first.}
        \label{fig:results-reposition-other-o}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \begin{center}
            \includegraphics[width=1.0\linewidth]{../scripts/images/chry_12_1.0_f_m_[f:6,].png}
        \end{center}
        \caption{Most common prediction.}
        \label{fig:results-reposition-other-m}
    \end{subfigure}
    \caption{Information of the current symbol and entropy as the file is processed, varying the pointer reposition strategy.}
    \label{fig:results-reposition-other}
\end{figure}

\subsection{Copy pointer reposition threshold}

A copy pointer is used until the predictions are deemed to not be worthwhile.
After that point, the copy pointer is repositioned taking the current pattern into account.
The decision of when to reposition the copy pointer can be done using different approaches.

We developed 3 strategies, all of which only factor in the prediction probability over time:
\begin{itemize}
    \item \textbf{Static}: a static probability threshold value. If the prediction probability falls below this value, then the pointer is repositioned.
    \item \textbf{Successive fails}: if the number of successive prediction misses is larger than a given value, then the pointer is repositioned.
    In this case, the ``successive fails'' are evaluated using a non-negative counter of misses that is updated as predictions are made; whenever there is a miss, the counter increases, and decreases otherwise.
    \item \textbf{Rate of change}: if the absolute difference between the current prediction probability and the previous prediction probability is below a threshold then the pointer is repositioned.
\end{itemize}

Each strategy has a threshold value associated with them, and they are passed as parameters along with the specified strategy.
These thresholds can also be combined together, indicating whether the pointer should be repositioned when at least one of the strategies reports that their threshold was surpassed.

\subsubsection{Results}

The results were initially evaluated for each strategy in isolation.
After determining the best performing threshold values for each approach, they were combined among themselves, testing the 4 possible combinations.

\dots

\subsection{Verbose output}
\label{sec:copy-model-verbose}

As a sidenote, the model offers the possibility of outputting the probability distributions and predicitons at each step, or of presenting the overall progress of processing the input file.

For the modes outputting the probabilities, the output can be in human-readable format, presenting whether the reported probabilites were from a guess or a prediction hit/miss in colored output.
Alternatively, the output can be presented as comma-separated values with less extraneous characters in machine-readable format, which is useful for data analysis afterwards.


\section{Text generator}
\label{sec:text-generator}

The Generator module makes use of the functionalities offered by the CPM module.
This module uses a Context Model calculated from a received text to calculate the
next characters based on a context phrase. 

It contains two main methods, the \verb|firstPass()|, that reads the training file and makes an initial pass through the text to calculate the frequencies of the characters.
This method also makes a context model based on the training file, and the size of the context is defined by the input string. 

The \verb|cpm_gen()| method uses the context model in the first pass to generate the text.
When we try to generate the next symbol, we read the current context, which is the size of the input string and we check the context model for that context. If we have that context in the model, we get the list of possible next symbols, and their counts. Then, we generate a random value between 0 and the sum of the counts, and we check where that value lands between all the ranges of counts of events of that context. The symbol that is in the range of the random value is the next symbol of the text.
If we don't have that context in the model, we get the list of all possible symbols in the alphabet, and their distribution, and we generate a random value between 0 and the sum of the distributions, and we check where that value lands between all the ranges of distributions of events of that context. The symbol that is in the range of the random value is the next symbol of the text. 

We also have 2 other input parameters that modify the behavior of the generator. The first one is allowing the model to train itself, while generating the text. This is done by updating the context model with the generated text and the distibution of symbols in the alphabet.
The second parameter is to allow all lowercase characters to be generated, this is done by converting all the characters to lowercase while reading the training file. This parameter provides a better context model without needing a big training file, but it also makes the text generated to be all lowercase.


\subsection{Results}

To better understand the Generator module and how different initial string, witch is our K and the parameters influence
text generation, the Generator module was trained with "othello.txt" wich is a very large text, varying initial string and the parameters, but
maintaining the seed value of 123, with an output limit size of 10 000 characters.
All the generated texts can be seen below

Results for the text generator without -l or -t parameters:

\hfill

Listing 1: Generator output for train file "othello.txt" with initial string "T" and without -l or -t parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
An atim Lobewast t ile: Ohincishy'diseapon'treanne s b'tcrod (mes howintene Wim
----------------------------------------------------------
\end{lstlisting}

Listing 2: Generator output for train file "othello.txt" with initial string "Th" and without -l or -t parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
I lour laight, lou sle orts soper shenle,
----------------------------------------------------------
\end{lstlisting}

Listing 3: Generator output for train file "othello.txt" with initial string "The" and without -l or -t parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
She the despeake ship my
----------------------------------------------------------
\end{lstlisting}

Listing 4: Generator output for train file "othello.txt" with initial string "The " and without -l or -t parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
The Token Wensday most the Moone you know:
----------------------------------------------------------
\end{lstlisting}

Listing 5: Generator output for train file "othello.txt" with initial string "The t" and without -l or -t parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
Enter Iago? Othe. What common his deluding her fantasie,
----------------------------------------------------------
\end{lstlisting}

Listing 6: Generator output for train file "othello.txt" with initial string "The te" and without -l or -t parameters
\\Num guesses: 590

\begin{lstlisting}
----------------------------------------------------------
For if such tricke vpon the error of this

fo' ofalMahetse n hrtuifo ie oeigmn   odeas' ) mHoe i i, 'lIt reodese:.se
----------------------------------------------------------
\end{lstlisting}

Listing 7: Generator output for train file "othello.txt" with initial string "The tes" and without -l or -t parameters
\\Num guesses: 2 604

\begin{lstlisting}
----------------------------------------------------------
all these Men, they do not, I do beseech you,

fo' ofalMahetse n hrtuifo ie oeigmn   odeas' ) mHoe i i, 'lIt reodese:.se
----------------------------------------------------------
\end{lstlisting}

Listing 8: Generator output for train file "othello.txt" with initial string "The test" and without -l or -t parameters
\\Num guesses: 10 000

\begin{lstlisting}
----------------------------------------------------------
doanelslhrCIh ieaY oo i yhnA:aBmygae
----------------------------------------------------------
\end{lstlisting}



Results for the text generator with -l parameter:


\hfill

Listing 1: Generator output for train file "othello.txt" with initial string "T" and with -l parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
win sss'd yousd   kecer hete, abeeaurugotieuthe y gr wine anem
----------------------------------------------------------
\end{lstlisting}

Listing 2: Generator output for train file "othello.txt" with initial string "Th" and with -l parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
thery prod sh me growelfe ore toponio.
----------------------------------------------------------
\end{lstlisting}

Listing 3: Generator output for train file "othello.txt" with initial string "The" and with -l parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
iago. and or pray yon. well'd be his drunke motifie sea
----------------------------------------------------------
\end{lstlisting}

Listing 4: Generator output for train file "othello.txt" with initial string "The " and with -l parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
you commited, and questie too hard wise: follow high. (prythee: an of a guilty th' confus'd,
----------------------------------------------------------
\end{lstlisting}

Listing 5: Generator output for train file "othello.txt" with initial string "The t" and with -l parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
cassio. aunciant, be abus'd: exclaimed. are about me filles vs a most graue.
 she speake heauen forget moe tapers.
----------------------------------------------------------
\end{lstlisting}

Listing 6: Generator output for train file "othello.txt" with initial string "The te" and with -l parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
and made of the bride, and easie taskes.
----------------------------------------------------------
\end{lstlisting}

Listing 7: Generator output for train file "othello.txt" with initial string "The tes" and with -l parameter
\\Num guesses: 2 191

\begin{lstlisting}
----------------------------------------------------------
make it a darling, like death, and her husband?

hr' ohdnadietse n istulho lh oelhnn   oeebt' ) naog i i, 'mat reoehsg:.sg
----------------------------------------------------------
\end{lstlisting}

Listing 8: Generator output for train file "othello.txt" with initial string "The test" and with -l parameter
\\Num guesses: 10 000

\begin{lstlisting}
----------------------------------------------------------
wi:  esmlen,ntd,  bri , so. eo e,alerc armeeiohitsdcslbo,isodt g ittus lhitw eh are eori.h he?
----------------------------------------------------------
\end{lstlisting}



Results for the text generator with -t parameter:


\hfill

Listing 1: Generator output for train file "othello.txt" with initial string "T" and with -t parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
Geee mey atoountineenet: Hat he Tuengo. Cat agagngile onifouph m y te, werellas te
----------------------------------------------------------
\end{lstlisting}

Listing 2: Generator output for train file "othello.txt" with initial string "Th" and with -t parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
Othe nand he sh your Lore Beat inke dere onclonge cozent,
----------------------------------------------------------
\end{lstlisting}

Listing 3: Generator output for train file "othello.txt" with initial string "The" and with -t parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
For lood my liue dingry. Thou haue State such my how I am do aboue lip:
----------------------------------------------------------
\end{lstlisting}

Listing 4: Generator output for train file "othello.txt" with initial string "The " and with -t parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
When haue was high. Haue you; a which, and adie: and sold not man haue that forcement 
that is euill she heere enemy in
----------------------------------------------------------
\end{lstlisting}

Listing 5: Generator output for train file "othello.txt" with initial string "The t" and with -t parameter
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
When haue was high. Haue you; a which, and adie:
 and sold not man haue that forcement that is euill she heere enemy in
----------------------------------------------------------
\end{lstlisting}

Listing 6: Generator output for train file "othello.txt" with initial string "The te" and with -t parameter
\\Num guesses: 319

\begin{lstlisting}
----------------------------------------------------------
Iago. You shall play the palme: I, well in Spleene,

dvdr bo'WliuaoS o( toy iyol  l dai Cdri  inc snuhmto h 
----------------------------------------------------------
\end{lstlisting}

Listing 7: Generator output for train file "othello.txt" with initial string "The tes" and with -t parameter
\\Num guesses: 7 548

\begin{lstlisting}
----------------------------------------------------------
The Iustice of the place againe.

to  oe aa rhFa   ed v.otg n?- aes uu
----------------------------------------------------------
\end{lstlisting}

Listing 8: Generator output for train file "othello.txt" with initial string "The test" and with -t parameter
\\Num guesses: 10 000

\begin{lstlisting}
----------------------------------------------------------
eCLtmetakirayetotF eelt)nhc reioneeol s o tetmrt.dse u
----------------------------------------------------------
\end{lstlisting}


Results for the text generator with -t and -l parameters:


\hfill

Listing 1: Generator output for train file "othello.txt" with initial string "T" and with -t and -l parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
bandont ftey wewhellllazoou t bero t t mince iseerododof  anke cor
----------------------------------------------------------
\end{lstlisting}

Listing 2: Generator output for train file "othello.txt" with initial string "Th" and with -t and -l parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
theerem be wit dood, why me my bassinesdeme
----------------------------------------------------------
\end{lstlisting}

Listing 3: Generator output for train file "othello.txt" with initial string "The" and with -t and -l parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
iago. not isleepe to madeliuillainstrankes
----------------------------------------------------------
\end{lstlisting}

Listing 4: Generator output for train file "othello.txt" with initial string "The " and with -t and -l parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
return'd againe to her grant the way. set it, and my sould he diuell.
----------------------------------------------------------
\end{lstlisting}

Listing 5: Generator output for train file "othello.txt" with initial string "The t" and with -t and -l parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
at blood beene me by and her officer iago. you shall my husband
----------------------------------------------------------
\end{lstlisting}

Listing 6: Generator output for train file "othello.txt" with initial string "The te" and with -t and -l parameters
\\Num guesses: 0

\begin{lstlisting}
----------------------------------------------------------
steep'd, to enflame it be true: or foredoes me without more the strongly to you:
 for by this is othello, desdemon?
----------------------------------------------------------
\end{lstlisting}

Listing 7: Generator output for train file "othello.txt" with initial string "The tes" and with -t and -l parameters
\\Num guesses: 7 349

\begin{lstlisting}
----------------------------------------------------------
e ,ulyirn et ai telott,tmnhel  n uoa  oor ydo   mofe   de.bowsaege ntdhoee 
----------------------------------------------------------
\end{lstlisting}

Listing 8: Generator output for train file "othello.txt" with initial string "The test" and with -t and -l parameters
\\Num guesses: 10 000

\begin{lstlisting}
----------------------------------------------------------
easerh w : p ht tn-lyerlehm eos shn snsn ovssniro h  fanmdi'l
----------------------------------------------------------
\end{lstlisting}

\section{Conclusion}
\label{sec:conclusion}

% TODO VERIFICAR ESTA INTRO
From the development of this project, it was possible to conclude that the Copy
Model is a successful and effective model to store occurrences of events after certain contexts
and to allow the calculation of probabilities of these events happening. In turn, this allows
the generation of new text based on training examples and on the information that the
model provides.

\subsection{Copy Model}

\subsection{CPM_gen}


By looking at the results we can conclude that the initial string, which is our K, is the parameter that influences our results the most.
This happens because a very small K generates words that don't make sense, and a very large K will reduce the amount of entries 
in the Hash Table which will lead to more guesses. The initial string should also take into account the language of the training file,
and should contain normal words, not names nor a special words. 

The training file should be as large as possible, to have the most accurate model possible. Because the bigger the file, the more 
information we have in the Hash Table to calculate the probabilities of the next symbol.

The -l parameter allows the generator to generate a text with only lower case letters, and is better overall and great for smaller input texts,
although it compromises the quality of the text generated due to being only lower case letters. 
The reason behind this is that the Hash Table will have less entries, and the probabilities will be more accurate.

The -t parameter allows the generator to train itself, and it lowers the number of guesses, but at the cost of that guess not making sense,
and then it will make the same error again. This parameter is better for larger input texts, because it will have more entries in the Hash Table,
and therefore decreasing the number of guesses and increasing the accuracy of the probabilities.

Using the double parameters it behaves similarly to the -t parameter, but it will have less guesses and have a slightly better accuracy at generating text that makes sense.
Although this small increase in accuracy is only noticeable in our results in the "The te" and "The tes" initial strings.
This makes it a better option than the -t option alone but it compromises the quality of the text generated due to being only lower case letters.

Also to denote that, the smaller the initial string, the more time it takes to produce an output, due to the size of each entry of 
the Hash Table increasing as the initial string size decreases. 


\section{References}
\bibliography{refs}
\bibliographystyle{IEEEtran}

\end{document}


